# Emotion Recognition Dataset Preprocessing and Analysis

This branch contains the Jupyter Notebook (`TextPreprocessing.ipynb`) that outlines the preprocessing steps applied to a large text dataset for a **Multi-Class Emotion Recognition** task.

The original dataset is sourced from **Kaggle**: [**emotions-dataset**](https://www.kaggle.com/datasets/bhavikjikadara/emotions-dataset).


## ðŸ’¾ Dataset Overview

> The dataset consists of text entries, each labeled with one of six primary emotions.

### Emotion Classes and Mapping:
| Integer Label | Emotion Class |
| :-----------: | :-----------: |
| **0** | Sadness |
| **1** | Happy (Joy) |
| **2** | Love |
| **3** | Anger |
| **4** | Fear |
| **5** | Surprise |


## ðŸ’¯ Null & Duplicated Values Handling

$$
\text{Initial Entries: } 416,809 \quad \xrightarrow{\substack{\text{Remove Duplicates} \\ (-686)}} \quad \text{Intermediate Entries: } 416,123 \quad \xrightarrow{\substack{\text{Remove Nulls} \\ (-15)}} \quad \text{Final Processed Entries: } 416,108
$$


## ðŸ§¹ Text Preprocessing Steps

The preprocessing pipeline was designed to clean the raw text data (`text` column) and prepare it for effective feature extraction and subsequent machine learning modeling.

### 1. Lowercasing
> All text was converted to **lowercase** to ensure consistency and prevent the same word capitalized differently from being treated as distinct tokens.

### 2. Contraction Removal (Specific)
> The notebook targeted contractions ending in **'`n t`'** (e.g., `don t`, `can t`, `didn t`) and replaced them with a space.

### 3. Stopword Removal
> Common English **stopwords** (e.g., 'the', 'a', 'is', 'on') were removed using the `nltk.corpus.stopwords` list, as these words typically do not contribute significant semantic meaning for emotion classification.

### 4. Remaining Contraction/Abbreviation Removal
> Further common contracted or abbreviated forms (`ive`, `im`, `its`, `dont`) that remained after step 2 were identified and removed, replacing them with a space.

### 5. Removal of Single Letters
> Any isolated **single letters** (which often result from the cleaning of contractions and stopwords, like 'w' or 'a') were removed to eliminate noise.

### 6. Tokenization and Lemmatization
> The cleaned text was **tokenized** (split into individual words), and then **lemmatized** using the `WordNetLemmatizer` with the part-of-speech set to verb (`pos="v"`). Lemmatization converts words back to their base or dictionary form **(e.g., 'feeling' to 'feel', 'started' to 'start')**.

### 7. Handling Null/Empty Values
> Rows where the `CleanedText` column became an empty string (`""`) after preprocessing were **dropped** from the dataset.

### 8. Final Feature Extraction (Word Count Comparison)
> The notebook concluded by comparing the total number of unique words before and after preprocessing:



## ðŸ“Š Feature Engineering (TF-IDF)

The preprocessed text data (`CleanedText`) was then transformed into a numerical format using **TF-IDF (Term Frequency-Inverse Document Frequency)** for use in a machine learning model.

* **Vectorizer Used:** `TfidfVectorizer`
* **Train/Test Split:** The dataset was split into training and testing sets with a `test_size=0.3` (30% for testing) and `random_state=42`.
* **Number of Extracted Features (Tokens):** **54,194** features were generated by the TF-IDF vectorizer on the training data.

## âœ… Effect of Text Preprocessing and Cleaning
| Stage | Column | Number of Unique Words |
| :---: | :---: | :---: |
| **Before Preprocessing** | `text` | **75,303** |
| **After Preprocessing** | `CleanedText` | **64,719** |

> This reduction of over 10,000 unique words demonstrates the effectiveness of the cleaning and normalization steps in reducing the vocabulary size and preparing the data for the next phase.


## ðŸ’¾ Output Files

The results of the preprocessing are saved into the following CSV files:

* `Cleaned_Emotions_4.csv`: Includes the original `text` and `label` columns, along with the new `CleanedText` and `Emotion` (mapped name) columns.
  
* `Cleaned_Emotions_2.csv`: Includes only the final `CleanedText` and `Emotion` columns, ready for modeling.
